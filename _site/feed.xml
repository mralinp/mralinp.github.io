<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-06-22T01:22:32+04:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">mralinp</title><subtitle>My awesome blog</subtitle><entry><title type="html">Lenet5 from scratch</title><link href="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/21/lenet5.html" rel="alternate" type="text/html" title="Lenet5 from scratch" /><published>2022-06-21T00:02:23+04:30</published><updated>2022-06-21T00:02:23+04:30</updated><id>http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/21/lenet5</id><content type="html" xml:base="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/21/lenet5.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>
<p>LeNet was introduced in the research paper “Gradient-Based Learning Applied To Document Recognition” in the year 1998 by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Many of the listed authors of the paper have gone on to provide several significant academic contributions to the field of deep learning.</p>
<p align="center">
    <img src="/assets/images/lenet5/authors.png" />
</p>
<!-- Image -->
<p>This article will introduce the LeNet-5 CNN architecture as described in the original paper, along with the implementation of the architecture using TensorFlow 2.0.</p>

<p>This article will then conclude with the utilization of the implemented LeNet-5 CNN for the classification of images from the MNIST dataset.</p>

<p>What to find in this article:</p>
<ul>
  <li>Understanding of components within a convolutional neural network</li>
  <li>Key definitions of terms commonly used in deep learning and machine learning</li>
  <li>Understanding of LeNet-5 as presented in the original research paper</li>
  <li>Implementation of a neural network using TensorFlow and Keras</li>
</ul>

<p>The content in this article is written for Deep learning and Machine Learning students of all levels. For those who are eager to get coding, scroll down to the ‘LeNet-5 TensorFlow Implementation’ section.</p>

<h1 id="2-convolutional-neural-networks">2. Convolutional Neural Networks</h1>
<p>Convolutional Neural Networks is the standard form of neural network architecture for solving tasks associated with images. Solutions for tasks such as object detection, face detection, pose estimation and more all have CNN architecture variants.</p>

<p>A few characteristics of the CNN architecture makes them more favourable in several computer vision tasks. I have written previous articles that dive into each characteristic.</p>

<ul>
  <li>Local respective fields</li>
  <li>Sub-sampling</li>
  <li>Weight sharing</li>
</ul>

<ol>
  <li>LeNet-5
LeNet-5 CNN architecture is made up of 7 layers. The layer composition consists of 3 convolutional layers, 2 subsampling layers and 2 fully connected layers.</li>
</ol>

<p align="center">
    <img src="/assets/images/lenet5/arch.png" />
</p>

<p>The diagram above shows a depiction of the LeNet-5 architecture, as illustrated in the original paper.</p>

<p>The first layer is the input layer — this is generally not considered a layer of the network as nothing is learnt in this layer. The input layer is built to take in 32x32, and these are the dimensions of images that are passed into the next layer. Those who are familiar with the MNIST dataset will be aware that the MNIST dataset images have the dimensions 28x28. To get the MNIST images dimension to the meet the requirements of the input layer, the 28x28 images are padded.</p>

<p>The grayscale images used in the research paper had their pixel values normalized from 0 to 255, to values between -0.1 and 1.175. The reason for normalization is to ensure that the batch of images have a mean of 0 and a standard deviation of 1, the benefits of this is seen in the reduction in the amount of training time. In the image classification with LeNet-5 example below, we’ll be normalizing the pixel values of the images to take on values between 0 to 1.</p>

<p><strong>The LeNet-5 architecture utilizes two significant types of layer construct: convolutional layers and subsampling layers.</strong></p>

<ul>
  <li>Convolution layers</li>
  <li>Sub-sampling layers</li>
</ul>

<p>Within the research paper and the image below, convolutional layers are identified with the ‘Cx’, and subsampling layers are identified with ‘Sx’, where ‘x’ is the sequential position of the layer within the architecture. ‘Fx’ is used to identify fully connected layers. This method of layer identification can be seen in the image above.</p>

<p>The official first layer convolutional layer C1 produces as output 6 feature maps, and has a kernel size of 5x5. The kernel/filter is the name given to the window that contains the weight values that are utilized during the convolution of the weight values with the input values. 5x5 is also indicative of the local receptive field size each unit or neuron within a convolutional layer. The dimensions of the six feature maps the first convolution layer produces are 28x28.</p>

<p>A subsampling layer ‘S2’ follows the ‘C1’ layer’. The ‘S2’ layer halves the dimension of the feature maps it receives from the previous layer; this is known commonly as downsampling.</p>

<p>The ‘S2’ layer also produces 6 feature maps, each one corresponding to the feature maps passed as input from the previous layer. This link contains more information on subsampling layers.</p>

<p>More information on the rest of the LeNet-5 layers is covered in the implementation section.
Below is a table that summarises the key features of each layer:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Layer name</th>
      <th style="text-align: center">Input</th>
      <th style="text-align: center">Kernel size</th>
      <th style="text-align: center">Output</th>
      <th style="text-align: center">Activation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Input</td>
      <td style="text-align: center">28x28x1</td>
      <td style="text-align: center">None</td>
      <td style="text-align: center">28x28x1</td>
      <td style="text-align: center">Relu</td>
    </tr>
    <tr>
      <td style="text-align: center">Convolution 1</td>
      <td style="text-align: center">28x28x1</td>
      <td style="text-align: center">5x5</td>
      <td style="text-align: center">24x24x6</td>
      <td style="text-align: center">Relu</td>
    </tr>
    <tr>
      <td style="text-align: center">Max pooling 1</td>
      <td style="text-align: center">24x24x6</td>
      <td style="text-align: center">2x2</td>
      <td style="text-align: center">12x12x6</td>
      <td style="text-align: center">Relu</td>
    </tr>
    <tr>
      <td style="text-align: center">Convolution 2</td>
      <td style="text-align: center">12x12x6</td>
      <td style="text-align: center">5x5</td>
      <td style="text-align: center">8x8x16</td>
      <td style="text-align: center">Relu</td>
    </tr>
    <tr>
      <td style="text-align: center">Max pooling 2</td>
      <td style="text-align: center">8x8x16</td>
      <td style="text-align: center">1x1</td>
      <td style="text-align: center">4x4x16</td>
      <td style="text-align: center">Relu</td>
    </tr>
    <tr>
      <td style="text-align: center">Flatten</td>
      <td style="text-align: center">4x4x16</td>
      <td style="text-align: center">None</td>
      <td style="text-align: center">256x1</td>
      <td style="text-align: center">None</td>
    </tr>
    <tr>
      <td style="text-align: center">Dense 1</td>
      <td style="text-align: center">256x1</td>
      <td style="text-align: center">None</td>
      <td style="text-align: center">120x1</td>
      <td style="text-align: center">Relu</td>
    </tr>
    <tr>
      <td style="text-align: center">Dense 2</td>
      <td style="text-align: center">120x1</td>
      <td style="text-align: center">None</td>
      <td style="text-align: center">84x1</td>
      <td style="text-align: center">Relu</td>
    </tr>
    <tr>
      <td style="text-align: center">Dense 3</td>
      <td style="text-align: center">84x1</td>
      <td style="text-align: center">None</td>
      <td style="text-align: center">10x1</td>
      <td style="text-align: center">Softmax</td>
    </tr>
  </tbody>
</table>

<p><br />
We begin implementation by importing the libraries we will be utilizing:</p>

<ul>
  <li>TensorFlow: An open-source platform for the implementation, training, and deployment of machine learning models.</li>
  <li>Keras: An open-source library used for the implementation of neural network architectures that run on both CPUs and GPUs.</li>
  <li>Numpy: A library for numerical computation with n-dimensional arrays.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">optimizers</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>
<p>Next, we load the MNIST dataset using the Keras library. The Keras library has a suite of datasets readily available for use with easy accessibility.</p>

<p>We are also required to partition the dataset into testing, validation and training. Here are some quick descriptions of each partition category.</p>

<ul>
  <li>Training Dataset: This is the group of our dataset used to train the neural network directly. Training data refers to the dataset partition exposed to the neural network during training.</li>
  <li>Validation Dataset: This group of the dataset is utilized during training to assess the performance of the network at various iterations.</li>
  <li>Test Dataset: This partition of the dataset evaluates the performance of our network after the completion of the training phase.</li>
</ul>

<p>It is also required that the pixel intensity of the images within the dataset are normalized from the value range 0–255 to 0–1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">),</span> <span class="p">(</span><span class="n">xTest</span><span class="p">,</span> <span class="n">yTest</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">xTrain</span><span class="p">),</span> <span class="mi">9</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)):</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">xTrain</span><span class="p">[</span><span class="n">indexes</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">yTrain</span><span class="p">[</span><span class="n">indexes</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>In the code snippet above, we expand the dimensions of the training and dataset. The reason we do this is that during the training and evaluation phases, the network expects the images to be presented within batches; the extra dimension is representative of the numbers of images in a batch.</p>

<p>The code below is the main part where we implement the actual LeNet-5 based neural network. Keras provides tools required to implement the classification model. Keras presents a Sequential API for stacking layers of the neural network on top of each other.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">numClasses</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Input</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">numClasses</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">leNet5</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="n">leNet5</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="p">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(),</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s">"accuracy"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">leNet5</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<pre><code class="language-output">Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 28, 28, 1)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 24, 24, 6)         156       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 12, 12, 6)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 8, 8, 16)          2416      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 4, 4, 16)          0         
_________________________________________________________________
flatten (Flatten)            (None, 256)               0         
_________________________________________________________________
dense (Dense)                (None, 120)               30840     
_________________________________________________________________
dense_1 (Dense)              (None, 84)                10164     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                850       
=================================================================
Total params: 44,426
Trainable params: 44,426
Non-trainable params: 0
</code></pre>

<p>We first assign the variable <code class="language-plaintext highlighter-rouge">lenet_5_model</code> to an instance of the tf.keras.Sequential class constructor. Within the class constructor, we then proceed to define the layers within our model.</p>

<p>The C1 layer is defined by the <code class="language-plaintext highlighter-rouge">linekeras.layers.Conv2D(6, kernel_size=5, strides=1, activation='tanh', input_shape=train_x[0].shape, padding='same')</code>. We are using the <code class="language-plaintext highlighter-rouge">tf.keras.layers.Conv2D</code> class to construct the convolutional layers within the network. We pass a couple of arguments which are described here.</p>

<ul>
  <li>Activation Function: A mathematical operation that transforms the result or signals of neurons into a normalized output. An activation function is a component of a neural network that introduces non-linearity within the network. The inclusion of the activation function enables the neural network to have greater representational power and solve complex functions.</li>
</ul>

<p>The rest of the convolutional layers follow the same layer definition as C1 with some different values entered for the arguments.</p>

<p>In the original paper where the LeNet-5 architecture was introduced, subsampling layers were utilized. Within the subsampling layer the average of the pixel values that fall within the 2x2 pooling window was taken, after that, the value is multiplied with a coefficient value. A bias is added to the final result, and all this is done before the values are passed through the activation function.</p>

<p>But in our implemented LeNet-5 neural network, we’re utilizing the tf.keras.layers.AveragePooling2D constructor. We don’ t pass any arguments into the constructor as some default values for the required arguments are initialized when the constructor is called. Remember that the pooling layer role within the network is to downsample the feature maps as they move through the network.</p>

<p>There are two more types of layers within the network, the flatten layer and the dense layers.</p>

<p>The flatten layer is created with the class constructor tf.keras.layers.Flatten.</p>

<p>The purpose of this layer is to transform its input to a 1-dimensional array that can be fed into the subsequent dense layers.</p>

<p>The dense layers have a specified number of units or neurons within each layer, F6 has 84, while the output layer has ten units.</p>

<p>The last dense layer has ten units that correspond to the number of classes that are within the MNIST dataset. The activation function for the output layer is a softmax activation function.</p>

<ul>
  <li>Softmax: An activation function that is utilized to derive the probability distribution of a set of numbers within an input vector. The output of a softmax activation function is a vector in which its set of values represents the probability of an occurrence of a class/event. The values within the vector all add up to 1.</li>
</ul>

<p>Keras provides the ‘compile’ method through the model object we have instantiated earlier. The compile function enables the actual building of the model we have implemented behind the scene with some additional characteristics such as the loss function, optimizer, and metrics.</p>

<p>To train the network, we utilize a loss function that calculates the difference between the predicted values provided by the network and actual values of the training data.</p>

<p>The loss values accompanied by an optimization algorithm(Adam) facilitates the number of changes made to the weights within the network. Supporting factors such as momentum and learning rate schedule, provide the ideal environment to enable the network training to converge, herby getting the loss values as close to zero as possible.</p>

<p>During training, we’ll also validate our model after every epoch with the valuation dataset partition created earlier</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">leNet5</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xTrain</span><span class="p">,</span> <span class="n">yTrain</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">xTest</span><span class="p">,</span><span class="n">yTest</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">leNet5</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">xTest</span><span class="p">,</span> <span class="n">yTest</span><span class="p">)</span>
</code></pre></div></div>
<pre><code class="language-output">Epoch 1/10
938/938 [==============================] - 10s 10ms/step - loss: 1.1559 - accuracy: 0.7988 - val_loss: 0.1056 - val_accuracy: 0.9681
Epoch 2/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0982 - accuracy: 0.9690 - val_loss: 0.0759 - val_accuracy: 0.9763
Epoch 3/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0595 - accuracy: 0.9809 - val_loss: 0.0706 - val_accuracy: 0.9792
Epoch 4/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0545 - accuracy: 0.9836 - val_loss: 0.0723 - val_accuracy: 0.9770
Epoch 5/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0472 - accuracy: 0.9856 - val_loss: 0.0594 - val_accuracy: 0.9825
Epoch 6/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0364 - accuracy: 0.9877 - val_loss: 0.0532 - val_accuracy: 0.9850
Epoch 7/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0358 - accuracy: 0.9887 - val_loss: 0.0813 - val_accuracy: 0.9776
Epoch 8/10
938/938 [==============================] - 10s 10ms/step - loss: 0.0333 - accuracy: 0.9895 - val_loss: 0.0682 - val_accuracy: 0.9829
Epoch 9/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0618 - val_accuracy: 0.9839
Epoch 10/10
938/938 [==============================] - 9s 10ms/step - loss: 0.0265 - accuracy: 0.9913 - val_loss: 0.0729 - val_accuracy: 0.9835
313/313 [==============================] - 1s 3ms/step - loss: 0.0729 - accuracy: 0.9835

[0.07292196899652481, 0.9835000038146973]
</code></pre>
<p>After training, you will notice that your model achieves a validation accuracy of over 90%. But for a more explicit verification of the performance of the model on an unseen dataset, we will evaluate the trained model on the test dataset partition created earlier.</p>

<p>After training my model, I was able to achieve 98% accuracy on the test dataset, which is quite useful for such a simple network.</p>

<h1 id="analyze-the-model-training-and-performance">Analyze the model training and performance</h1>

<h2 id="confusion-matrix">Confusion matrix</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">yPred</span> <span class="o">=</span> <span class="n">leNet5</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xTest</span><span class="p">)</span>
<span class="n">yPred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">yPred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">conf</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">yTest</span><span class="p">,</span> <span class="n">yPred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">yPred</span><span class="p">,</span> <span class="n">yTest</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy Score:"</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Confusion matrix"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="accuracy-and-error-plots">Accuracy and error plots</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_train</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">loss_val</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss_train</span><span class="p">,</span> <span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>You can see the complete project code in my <a href="https://github.com/mralinp/cnn-networks/tree/main/lenet5"><strong>github repo</strong></a></p>

<p>I hope you found the article useful.</p>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><category term="ai" /><category term="machine-learning" /><category term="deep-learning" /><summary type="html"><![CDATA[1. Intro LeNet was introduced in the research paper “Gradient-Based Learning Applied To Document Recognition” in the year 1998 by Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Many of the listed authors of the paper have gone on to provide several significant academic contributions to the field of deep learning. This article will introduce the LeNet-5 CNN architecture as described in the original paper, along with the implementation of the architecture using TensorFlow 2.0.]]></summary></entry><entry><title type="html">What is an AutoEncoder?</title><link href="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/19/auto-encoder.html" rel="alternate" type="text/html" title="What is an AutoEncoder?" /><published>2022-06-19T13:21:13+04:30</published><updated>2022-06-19T13:21:13+04:30</updated><id>http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/19/auto-encoder</id><content type="html" xml:base="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/19/auto-encoder.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>
<p>AutoEncoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.
AutoEncoder, by design, reduces data dimensions by learning how to ignore the noise in the data.
Here is an example of the input/output image from the MNIST dataset to an AutoEncoder.</p>

<p align="center">
  <img src="/assets/images/auto-encoder/ae-arch.jpeg" />
</p>

<h2 id="11-autoencoder-components">1.1 AutoEncoder Components:</h2>
<p>Autoencoders consists of 4 main parts:</p>
<ol>
  <li>
    <p><strong>Encoder</strong>: In which the model learns how to reduce the input dimensions and compress the input data into an encoded representation.</p>
  </li>
  <li>
    <p><strong>Bottleneck</strong>: which is the layer that contains the compressed representation of the input data. This is the lowest possible dimensions of the input data.</p>
  </li>
  <li>
    <p><strong>Decoder</strong>: In which the model learns how to reconstruct the data from the encoded representation to be as close to the original input as possible.</p>
  </li>
  <li>
    <p><strong>Reconstruction Loss</strong>: This is the method that measures measure how well the decoder is performing and how close the output is to the original input.</p>
  </li>
</ol>

<p>The training then involves using back propagation in order to minimize the network’s reconstruction loss. You must be wondering why would I train a neural network just to output an image or data that is exactly the same as the input! This article will cover the most common use cases for Autoencoder. Let’s get started:</p>

<h2 id="12-autoencoder-architecture">1.2 AutoEncoder Architecture:</h2>
<p>The network architecture for autoencoders can vary between a simple FeedForward network, LSTM network or Convolutional Neural Network depending on the use case. We will explore some of those architectures in the new next few lines.</p>

<h1 id="2-autoencoder-for-anomaly-detection">2. Autoencoder for Anomaly Detection:</h1>
<p>There are many ways and techniques to detect anomalies and outliers. However, if you have correlated input data, the autoencoder method will work very well because the encoding operation relies on the correlated features to compress the data.</p>

<p>Let’s say that we have trained an autoencoder on the MNIST dataset. Using a simple FeedForward neural network, we can achieve this by building a simple 6 layers network as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">optimizers</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="p">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">val_x</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>

<span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span>  <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span>  <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span>    <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"bottleneck"</span><span class="p">))</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span>  <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span>  <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span>  <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">())</span>
<span class="n">trained_model</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">))</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">autoencoder</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s">'bottleneck'</span><span class="p">).</span><span class="n">output</span><span class="p">)</span>
<span class="n">encoded_data</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>  <span class="c1"># bottleneck representation
</span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>        <span class="c1"># reconstruction
</span><span class="n">encoding_dim</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># return the decoder
</span><span class="n">encoded_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,))</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">](</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">](</span><span class="n">decoder</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">decoder</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>
</code></pre></div></div>
<pre><code class="language-output">Train on 60000 samples, validate on 10000 samples
Epoch 1/10
60000/60000 [==============================] - 6s 103us/step - loss: 0.0757 - val_loss: 0.0505
Epoch 2/10
60000/60000 [==============================] - 6s 96us/step - loss: 0.0420 - val_loss: 0.0355
Epoch 3/10
60000/60000 [==============================] - 6s 95us/step - loss: 0.0331 - val_loss: 0.0301
Epoch 4/10
60000/60000 [==============================] - 6s 96us/step - loss: 0.0287 - val_loss: 0.0266
Epoch 5/10
60000/60000 [==============================] - 6s 95us/step - loss: 0.0259 - val_loss: 0.0244
Epoch 6/10
60000/60000 [==============================] - 6s 96us/step - loss: 0.0240 - val_loss: 0.0228
Epoch 7/10
60000/60000 [==============================] - 6s 95us/step - loss: 0.0226 - val_loss: 0.0216
Epoch 8/10
60000/60000 [==============================] - 6s 97us/step - loss: 0.0215 - val_loss: 0.0207
Epoch 9/10
60000/60000 [==============================] - 6s 96us/step - loss: 0.0207 - val_loss: 0.0199
Epoch 10/10
60000/60000 [==============================] - 6s 96us/step - loss: 0.0200 - val_loss: 0.0193
</code></pre>

<p>As you can see in the output, the last reconstruction loss/error for the validation set is 0.0193 which is great. Now, if I pass any normal image from the MNIST dataset, the reconstruction loss will be very low (&lt; 0.02) BUT if I tried to pass any other different image (outlier or anomaly), we will get a high reconstruction loss value because the network failed to reconstruct the image/input that is considered an anomaly.</p>

<p>Notice in the code above, you can use only the encoder part to compress some data or images and you can also only use the decoder part to decompress the data by loading the decoder layers.</p>

<p>Now, let’s do some anomaly detection. The code below uses two different images to predict the anomaly score (reconstruction error) using the autoencoder network we trained above. the first image is from the MNIST and the result is 5.43209. This means that the image is not an anomaly. The second image I used, is a completely random image that doesn’t belong to the training dataset and the results were: 6789.4907. This high error means that the image is an anomaly. The same concept applies to any type of dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.preprocessing</span> <span class="kn">import</span> <span class="n">image</span>
<span class="c1"># if the img.png is not one of the MNIST dataset that the model was trained on, the error will be very high.
</span><span class="n">img</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">load_img</span><span class="p">(</span><span class="s">"./img.png"</span><span class="p">,</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">color_mode</span> <span class="o">=</span> <span class="s">"grayscale"</span><span class="p">)</span>
<span class="n">input_img</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">input_img</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">)</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">autoencoder</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">target_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
</code></pre></div></div>
<h1 id="3-image-de-noising">3. Image de-noising:</h1>

<p>Denoising or noise reduction is the process of removing noise from a signal. This can be an image, audio or a document. You can train an Autoencoder network to learn how to remove noise from pictures. In order to try out this use case, let’s re-use the famous MNIST dataset and let’s create some synthetic noise in the dataset. The code below will simply add some noise to the dataset then plot a few pictures to make sure that we’ve successfully created them.</p>

<p align="center">
    <img src="/assets/images/auto-encoder/denoising.png" />
</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">noise_factor</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> 
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> 

<span class="n">x_train_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_train_noisy</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
<span class="n">x_test_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="c1">#Print one image to see the noise
</span><span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
</code></pre></div></div>

<p>The output of the code above is the image below, which is pretty noisy and fuzzy:</p>

<p align="center">
    <img src="/assets/images/auto-encoder/noisy-img.png" />
</p>

<p>In this example, let’s build a Convolutional Autoencoder Neural Network. I will walk through each line of building the network:</p>

<p>First, we define the input layer and the dimensions of the input data. MNIST dataset has images that are reshaped to be 28 X 28 in dimensions. Since the images are greyscaled, the colour channel of the image will be 1 so the shape is (28, 28, 1).</p>

<p>The second layer is the convolution layer, this layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. 32 is the number of output filters in the convolution and (3, 3) is the kernel size.</p>

<p>After each convolution layer, we use MaxPooling function to reduce the dimensions. The (28, 28, 32) is reduced by a factor of two so it will be (14, 14, 32) after the first MaxPooling then (7, 7, 32) after the second MaxPooling. This is the encoded representation of the image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_img</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">nn</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">)(</span><span class="n">input_img</span><span class="p">)</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">)(</span><span class="n">nn</span><span class="p">)</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">)(</span><span class="n">nn</span><span class="p">)</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">)(</span><span class="n">nn</span><span class="p">)</span>
</code></pre></div></div>

<p>The code below is the reconstruction part of the original digits. This is where the network actually learns how to remove the noise from the input images. We use UpSampling function to rebuild the images to the original dimensions (28, 28)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">)(</span><span class="n">encoded</span><span class="p">)</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">UpSampling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">nn</span><span class="p">)</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">)(</span><span class="n">nn</span><span class="p">)</span>
<span class="n">nn</span> <span class="o">=</span> <span class="n">UpSampling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">nn</span><span class="p">)</span>
<span class="n">decoded</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">)(</span><span class="n">nn</span><span class="p">)</span>
</code></pre></div></div>

<p>Now, the last remaining step is to create the model, compile it then start the training. We do this by running:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">autoencoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_img</span><span class="p">,</span> <span class="n">decoded</span><span class="p">)</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'adadelta'</span><span class="p">,</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">)</span>
<span class="n">autoencoder</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_noisy</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span>
                <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test_noisy</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
</code></pre></div></div>

<p>After the training is complete, I try to pass one noisy image through the network and the results are quite impressive, the noise was completely removed:</p>

<p align="center">
    <img src="/assets/images/auto-encoder/gen-num-from-noise.png" />
</p>

<p>If you scale the ConvNet above, you can use it to denoise any type of images, audio or scanned documents.</p>

<p>In this part of the article, I covered two important use cases for autoencoders and I build two different neural network architectures — CNN and FeedForward. In part 2, I will cover another 2 important use cases for Autoencoders. The first one will be how to use autoencoder with a sequence of data by building an LSTM network and the second use case is a called Variational Autoencoder (VAE) which is mainly used in Generative Models and generating data or images. Stay tuned!</p>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><category term="ai" /><category term="machine-learning" /><category term="deep-learning" /><summary type="html"><![CDATA[1. Intro AutoEncoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible. AutoEncoder, by design, reduces data dimensions by learning how to ignore the noise in the data. Here is an example of the input/output image from the MNIST dataset to an AutoEncoder.]]></summary></entry><entry><title type="html">Adversarial attacks in deep learning</title><link href="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/16/adverserial-attack.html" rel="alternate" type="text/html" title="Adversarial attacks in deep learning" /><published>2022-06-16T21:02:05+04:30</published><updated>2022-06-16T21:02:05+04:30</updated><id>http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/16/adverserial-attack</id><content type="html" xml:base="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/06/16/adverserial-attack.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>
<p>Big Data powered machine learning and deep learning has yielded impressive advances in many fields. One example is the release of ImageNet consisting of more than 15 million labelled high-resolution images of 22,000 categories which revolutionized the field of computer vision. State-of-the-art models have already achieved a 98% top-five accuracy on the ImageNet dataset, so it seems as though these models are foolproof and that nothing can go wrong.</p>

<p>However, recent advances in adversarial training have found that this is an illusion. A good model misbehaves frequently when faced with adversarial examples. The image below illustrates the problem:</p>

<p><img src="/assets/images/adversarial-attack/1.png" alt="image" /></p>

<p>The model initially classifies the panda picture correctly, but when some noise, imperceptible to human beings, is injected into the picture, the resulting prediction of the model is changed to another animal, gibbon, even with such a high confidence. To us, it appears as if the initial and altered images are the same, although it is radically different to the model. This illustrates the threat these adversarial attacks pose — we may not perceive the difference so we cannot tell an adversarial attack as happened. Hence, although the output of the model may be altered, we cannot tell if the output is correct or incorrect.</p>

<p>This formed the motivation behind the talk for Professor Ling Liu’s keynote speech at the 2019 IEEE Big Data Conference, where she touched on types of adversarial attacks, how adversarial examples are generated, and how to combat against these attacks. Without further ado, I will get into the contents of her speech.</p>

<h1 id="table-of-contents">Table of contents</h1>
<ul>
  <li><a href="#1-intro">1. Introduction</a></li>
  <li><a href="#2-types-of-adversarial-attacks">2. Types of adversarial attacks</a></li>
  <li><a href="#3-how-are-adversarial-examples-generated">3. How are adversarial examples generated</a></li>
  <li><a href="#4-adversarial-perturbation">4. Adversarial perturbation</a></li>
  <li><a href="#5-black-box-vs-white-box-attacks">5. Black Box VS White Box Attacks</a>
    <ul>
      <li><a href="#51-black-box-attacks">5.1 Black box attacks</a></li>
      <li><a href="#52-white-box-attacks">5.2 White box attacks</a></li>
      <li><a href="#53-physical-attacks">5.3 Physical Attacks</a></li>
    </ul>
  </li>
</ul>

<h1 id="2-types-of-adversarial-attacks">2. Types of adversarial attacks</h1>

<p>Adversarial attacks are classified into two categories — targeted attacks and untargeted attacks.</p>

<p>The targeted attack has a target class, Y, that it wants the target model, M, to classify the image I of class X as. Hence, the goal of the targeted attack is to make M misclassify by predicting the adversarial example, I, as the intended target class Y instead of the true class X. On the other hand, the untargeted attack does not have a target class which it wants the model to classify the image as. Instead, the goal is simply to make the target model misclassify by predicting the adversarial example, I, as a class, other than the original class, X.
Researchers have found that in general, although untargeted attacks are not as good as targeted attacks, they take much less time. Targeted attacks, although more successful in altering the predictions of the model, come at a cost (time).</p>

<h1 id="3-how-are-adversarial-examples-generated">3. How are Adversarial Examples Generated</h1>

<p>Having understood the difference between targeted and untargeted attacks, we now come to the question of how these adversarial attacks are carried out. In a benign machine learning system, the training process seeks to minimize the loss between the target label and the predicted label, formulated mathematically as such:</p>

<!-- Image -->

<p>During the testing phase, the learned model is tested to determine how well it can predict the predicted label. Error is then calculated by the sum of the loss between the target label and the predicted label, formulated mathematically as such:
<!-- Image -->
In adversarial attacks, the following 2 steps are taken:</p>
<ol>
  <li>The query input is changed from the benign input x to \(x^\prime\).</li>
  <li>An attack goal is set such that the prediction outcome, \(H(x)\) is no longer \(y\). The loss is changed from \(L(H(x_i), y_i)\) to \(L(H(x_i), y^{\prime}_i)\) where \(y^{\prime}_i  \ne y_i\).</li>
</ol>

<h1 id="4-adversarial-perturbation">4. Adversarial Perturbation</h1>
<p>One way the query input is changed from x to x’ is through the method called “adversarial perturbation”, where the perturbation is computed such that the prediction will not be the same as the original label. For images, this can come in the form of pixel noise as we saw above with the panda example. Untargeted attacks have the single goal of maximizing the loss between H(x) and H(x’) until the prediction outcome is not y (the real label). Targeted attacks have an additional goal of not only maximizing the loss between H(x) and H(x’) but also to minimize the loss between H(x’) and y’ until H(x’) = y’ instead of y.</p>

<p>Adversarial perturbation can then be categorized into one-step and multi-step perturbation. As the names imply, the one-step perturbation only involves a single stage — add noise once and that is it. On the other hand, the multi-step perturbation is an iterative attack that makes small modifications to the input each time. Therefore, the one-step attack is fast but excessive noise may be added, hence making it easier for humans to detect the changes. Furthermore, it places more weight on the objective of maximizing loss between H(x) and H(x’) and less on minimizing the amount of perturbation. Conversely, the multi-step attack is more strategic as it introduces small amounts of perturbation at each time. However, this also means such an attack is computationally more expensive.</p>

<h1 id="5-black-box-vs-white-box-attacks">5. Black Box VS White Box Attacks</h1>
<p>Now that we have looked at how adversarial attacks are generated, some astute readers may realize one fundamental assumption these attacks take on — that the attack target prediction model, H, is known to the adversary. Only when the targeted model is known can it be compromised to generate adversarial examples by changing the input. However, attackers do not always know or have access to the targeted model. This may sound like a surefire way to ward off these adversarial attackers, but the truth is that black box attacks are also highly effective.
Black box attacks are based on the notion of transferability of adversarial examples — the phenomenon whereby adversarial examples, although generated to attack a surrogate model G, can achieve impressive results when attacking another model H. The steps taken are as follows:</p>
<ol>
  <li>The attack target prediction model H is privately trained and unknown to the adversary.</li>
  <li>A surrogate model G, which mimics H, is used to generate adversarial examples.</li>
  <li>By using the transferability of adversarial examples, black box attacks can be launched to attack H.</li>
</ol>

<p>This attack can be launched either with the training dataset being known or unknown. In the case where the dataset is known to the adversary, the model G can be trained on the same dataset as model H to mimic H.</p>

<p>When the training dataset is unknown however, adversaries can leverage on Membership Inference Attacks, whereby an attack model whose purpose is to distinguish the target model’s behavior on the training inputs from its behavior on the inputs that it did not encounter during training is trained. In essence, this turns into a classification problem to recognize differences in the target model’s predictions on the inputs that it trained on versus the inputs that it did not train on. This enables the adversary to obtain a better sense of the training dataset D which model H was trained on, enabling the attacker to generate a shadow dataset S on the basis of the true training dataset so as to train the surrogate model G. Having trained G on S where G mimics H and S mimics D, black box attacks can then be launched on H.</p>

<h2 id="51-black-box-attacks">5.1 Black Box Attacks</h2>
<p>Now that we have seen how black box attacks vary from white box attacks in that the target model H is unknown to the adversary, we will cover the various tactics used in black box attacks.</p>

<h2 id="52-white-box-attacks">5.2 White Box Attacks</h2>

<h2 id="53-physical-attacks">5.3 Physical Attacks</h2>
<p>One simple way in which the query input is changed from x to x’ is by simply adding something physically (eg. bright colour) to disturb the model. One example is how researchers at CMU added eyeglasses to a person in an attack against facial recognition models. The image below illustrates the attack:</p>

<p><img src="/assets/images/adversarial-attack/2.png" alt="image" /></p>

<p>The first row of images correspond to the original image modified by adding the eyeglasses, and the second row of images correspond to the impersonation targets, which are the intended misclassification targets. Just by adding the eyeglasses onto the original image, the facial recognition model was tricked into classifying the images on the top row as the images in the bottom row.</p>

<p>Another example comes from researchers at Google who added stickers to the input image to change the classification of the image, as illustrated by the image below:
<img src="/assets/images/adversarial-attack/3.png" alt="image" /></p>

<p>These examples show how effective such physical attacks can be.</p>

<h1 id="out-of-distribution-ood-attack">Out of Distribution (OOD) Attack</h1>
<p>Another way in which black box attacks are carried out is through out-of-distribution (OOD) attacks. The traditional assumption in machine learning is that all train and test examples are drawn independently from the same distribution. In an OOD attack, this assumption is exploited by providing images of a different distribution from the training dataset to the model, for example feeding TinyImageNet data into a CIFAR-10 classifier which would lead to an incorrect prediction with high confidence.</p>

<h1 id="how-can-we-trust-machine-learning">How Can We Trust Machine Learning?</h1>
<p>Now that we have taken a look at the various types of adversarial attacks, a natural question then comes — how can we trust our machine learning models if they are so susceptible to adversarial attacks?</p>

<p>One possible approach has been proposed by Chow et al. in 2019 in the paper titled “Denoising and Verification Cross-Layer Ensemble Against Black-box Adversarial Attacks”. The approach is centred around enabling machine learning systems to automatically detect adversarial attacks and then automatically repair them through the use of denoising and verification ensembles.</p>

<h1 id="denoising-ensembles">Denoising Ensembles</h1>
<p>First, input images have to pass through denoising ensembles that attempt different methods to remove any added noise to the image, for example adding Gaussian noise. Since the specific noise added to the image by the adversary is unknown to the defender, there is a need for an ensemble of denoisers to each attempt to remove each type of noise.</p>

<p>The image below shows the training process for the denoising autoencoder — the original image is injected with some noise that the attacker might inject, and the autoencoder tries to reconstruct the original uncorrupted image. In the training process, the objective is to reduce the reconstruction error between the reconstructed image and the original image.</p>

<p><img src="/assets/images/adversarial-attack/4.png" alt="image" /></p>

<p>By developing an ensemble of these autoencoders each trained to remove a specific type of noise, the hope is that the corrupted images would be sufficiently denoised such that it is close to the original uncorrupted image to allow for image classification.</p>

<h2 id="verification-ensemble">Verification Ensemble</h2>
<p>After the images have been denoised, they then go through a verification ensemble which reviews every denoised image produced by each denoiser and then classifies the denoised image. Each classifier in the verification ensemble classifies each denoised image, and the ensemble then votes to determine the final category the image belongs to. This means that although some images may not have been denoised the correct way in the denoising step, the verification ensemble votes on all the denoised images, thereby increasing the likelihood of making a more accurate prediction.</p>

<h2 id="diversity">Diversity</h2>
<p>Diversity of the denoisers and verifiers have found to be very important because firstly, adversarial attackers will get better at altering images so there is a need for a diverse group of denoisers that can handle a variety of corrupted images. Following this, there is also a need for verifiers to be diverse so they can generate a variety of classifications so that it would be difficult adversarial attackers to manipulate them just as how they have managed to manipulate normal classifiers that we trust and use so frequently in machine learning.</p>

<p>This remains an open problem because, after all these decisions by the various verifiers, there is still a final decision maker that needs to decide whose opinion to listen to. The final decision maker would need to preserve the diversity present in the ensemble, which is not an easy task to tackle.</p>

<h1 id="conclusion">Conclusion</h1>
<p>We have taken a look at various types of adversarial attacks as well as a promising method to defend against these attacks. This is definitely something to keep in mind when we implement machine learning models. Instead of blindly trusting the models to produce the correct results, we need to guard against these adversarial attacks and always think twice before we accept the decisions made by these models.</p>

<p>A huge thanks to Professor Liu for this enlightening keynote on this pressing problem in machine learning!</p>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><category term="ai" /><category term="machine-learning" /><category term="deep-learning" /><summary type="html"><![CDATA[1. Intro Big Data powered machine learning and deep learning has yielded impressive advances in many fields. One example is the release of ImageNet consisting of more than 15 million labelled high-resolution images of 22,000 categories which revolutionized the field of computer vision. State-of-the-art models have already achieved a 98% top-five accuracy on the ImageNet dataset, so it seems as though these models are foolproof and that nothing can go wrong.]]></summary></entry><entry><title type="html">Snake game</title><link href="http://localhost:4000/project/game/python/entry-level/2022/03/21/snake-game.html" rel="alternate" type="text/html" title="Snake game" /><published>2022-03-21T18:15:32+03:30</published><updated>2022-03-21T18:15:32+03:30</updated><id>http://localhost:4000/project/game/python/entry-level/2022/03/21/snake-game</id><content type="html" xml:base="http://localhost:4000/project/game/python/entry-level/2022/03/21/snake-game.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>
<p>Hello there, this is Ali speaking. When i started studying computer engineering at Shiraz university, CSE101 was the very primarly course that we should take to get familiar with the basic concepts of programming. They introduced <strong>python</strong> to us for the first time. Python is a very cool programming language that have very waste practical areas in almost any majors such as Data science, machine learning, web-development, game development, etc.</p>

<p>Now i’m using python on almost my every day of my academic career. So it was a good choice to begin programming and get familiar with the concepts of programming. On the other hand, in the first year of high-school i learned <strong>C</strong> programming as my first language. I was using C to solve mathematic problems and only for fun because I didn’t have any other source to learn, except our computer teacher and a very old resource which was based on <strong>Borland C++</strong> and was running on <strong>MS-DOS</strong>. That days we didn’t have the access to the internet on our home or school so the only way of learning things was reading books and asking questions. So to me the course CSE101 was the easiest course of my life. I never studied anything about the course silabuses but I was just learning everything possible about python that could found. <strong>Our final project was making a very simple snake game (like old school Nokia phones game) without using advanced libraries such as pyGame or other alternatives.</strong></p>

<h2 id="11-problem-statement">1.1 Problem statement:</h2>

<p>Develop a straightforward snake game such as the one on old-school Nokia phones. A single snake moves around the game world. Users can control the snake using arrows keys or W, A, S, and D to move UP, LEFT, DOWN, and RIGHT.</p>

<p>After a random time, it will be deployed an apple on a random spot on the map. If the snake eats the apple, its tail extends, and the player receives 100 points. The map can contain some walls or obstacles. If the snake hits a block or its tail, the game will be finished, and the player will lose the game.</p>

<p>By pressing the Esc key, the game pauses. You have to make the ability to store the highest score after the game finishes.</p>

<p>Any creative ideas and implementation that improve performance, game experience, and game appearance considers as bonuses.</p>

<h1 id="2-solution">2. Solution</h1>
<p>Did you read <strong><a href="#11-problem-statement">part 1.1</a></strong> carefully? As it could be found from the problem statement. We can break the problem into some parts and levels. First is to implement a moving object on the 2D game board which actually is our snake. As the game world is console and no graphical libraries are allowed here we need to think and consider about a simple world of char games. In this world, all objects in the game are nothing but characters. For example snake head can be displayed as the character ‘@’, its tail segments can be displayed with ‘o’ and Apples are displayed with ‘A’. At the end walls could be represented as ‘#’. First, we try to implement the basics of the game. The challenge here is how to print a character on a specific position of the screen.</p>

<h1 id="3-implementation">3. Implementation</h1>

<h1 id="4-final-words">4. Final words</h1>]]></content><author><name>Ali N. Parizi</name></author><category term="project" /><category term="game" /><category term="python" /><category term="entry-level" /><summary type="html"><![CDATA[1. Intro Hello there, this is Ali speaking. When i started studying computer engineering at Shiraz university, CSE101 was the very primarly course that we should take to get familiar with the basic concepts of programming. They introduced python to us for the first time. Python is a very cool programming language that have very waste practical areas in almost any majors such as Data science, machine learning, web-development, game development, etc.]]></summary></entry><entry><title type="html">Hello, world!</title><link href="http://localhost:4000/blog/2022/03/20/hello-world.html" rel="alternate" type="text/html" title="Hello, world!" /><published>2022-03-20T20:02:05+03:30</published><updated>2022-03-20T20:02:05+03:30</updated><id>http://localhost:4000/blog/2022/03/20/hello-world</id><content type="html" xml:base="http://localhost:4000/blog/2022/03/20/hello-world.html"><![CDATA[<p>This is my first post on the blog on the first day of the new century (1401-01-01 00:00:00 +330).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"Hello, world!"</span>
</code></pre></div></div>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><summary type="html"><![CDATA[This is my first post on the blog on the first day of the new century (1401-01-01 00:00:00 +330).]]></summary></entry></feed>