<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-21T16:04:56+03:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">mralinp</title><subtitle>My awesome blog</subtitle><entry><title type="html">Adversarial attacks in deep learning</title><link href="http://localhost:4000/blog/ai/machine-learning/deep-learning/2023/03/21/adverserial-attack.html" rel="alternate" type="text/html" title="Adversarial attacks in deep learning" /><published>2023-03-21T16:01:02+03:30</published><updated>2023-03-21T16:01:02+03:30</updated><id>http://localhost:4000/blog/ai/machine-learning/deep-learning/2023/03/21/adverserial-attack</id><content type="html" xml:base="http://localhost:4000/blog/ai/machine-learning/deep-learning/2023/03/21/adverserial-attack.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>
<p>Big Data powered machine learning and deep learning has yielded impressive advances in many fields. One example is the release of ImageNet consisting of more than 15 million labelled high-resolution images of 22,000 categories which revolutionized the field of computer vision. State-of-the-art models have already achieved a 98% top-five accuracy on the ImageNet dataset, so it seems as though these models are foolproof and that nothing can go wrong.</p>

<p>However, recent advances in adversarial training have found that this is an illusion. A good model misbehaves frequently when faced with adversarial examples. The image below illustrates the problem:</p>
<p align="center">
 <img src="/assets/images/adversarial-attack/1.png" />
</p>
<p>The model initially classifies the panda picture correctly, but when some noise, imperceptible to human beings, is injected into the picture, the resulting prediction of the model is changed to another animal, gibbon, even with such a high confidence. To us, it appears as if the initial and altered images are the same, although it is radically different to the model. This illustrates the threat these adversarial attacks pose — we may not perceive the difference so we cannot tell an adversarial attack as happened. Hence, although the output of the model may be altered, we cannot tell if the output is correct or incorrect.</p>

<p>This formed the motivation behind the talk for Professor Ling Liu’s keynote speech at the 2019 IEEE Big Data Conference, where she touched on types of adversarial attacks, how adversarial examples are generated, and how to combat against these attacks. Without further ado, I will get into the contents of her speech.</p>

<h1 id="2-types-of-adversarial-attacks">2. Types of adversarial attacks</h1>

<p>Adversarial attacks are classified into two categories — targeted attacks and untargeted attacks.</p>

<p>The targeted attack has a target class, Y, that it wants the target model, M, to classify the image I of class X as. Hence, the goal of the targeted attack is to make M misclassify by predicting the adversarial example, I, as the intended target class Y instead of the true class X. On the other hand, the untargeted attack does not have a target class which it wants the model to classify the image as. Instead, the goal is simply to make the target model misclassify by predicting the adversarial example, I, as a class, other than the original class, X.
Researchers have found that in general, although untargeted attacks are not as good as targeted attacks, they take much less time. Targeted attacks, although more successful in altering the predictions of the model, come at a cost (time).</p>

<h1 id="3-how-are-adversarial-examples-generated">3. How are Adversarial Examples Generated</h1>

<p>Having understood the difference between targeted and untargeted attacks, we now come to the question of how these adversarial attacks are carried out. In a benign machine learning system, the training process seeks to minimize the loss between the target label and the predicted label, formulated mathematically as such:</p>

<!-- Image -->

<p>During the testing phase, the learned model is tested to determine how well it can predict the predicted label. Error is then calculated by the sum of the loss between the target label and the predicted label, formulated mathematically as such:
<!-- Image -->
In adversarial attacks, the following 2 steps are taken:</p>
<ol>
  <li>The query input is changed from the benign input x to \(x^\prime\).</li>
  <li>An attack goal is set such that the prediction outcome, \(H(x)\) is no longer \(y\). The loss is changed from \(L(H(x_i), y_i)\) to \(L(H(x_i), y^{\prime}_i)\) where \(y^{\prime}_i  \ne y_i\).</li>
</ol>

<h1 id="4-adversarial-perturbation">4. Adversarial Perturbation</h1>
<p>One way the query input is changed from x to x’ is through the method called “adversarial perturbation”, where the perturbation is computed such that the prediction will not be the same as the original label. For images, this can come in the form of pixel noise as we saw above with the panda example. Untargeted attacks have the single goal of maximizing the loss between H(x) and H(x’) until the prediction outcome is not y (the real label). Targeted attacks have an additional goal of not only maximizing the loss between H(x) and H(x’) but also to minimize the loss between H(x’) and y’ until H(x’) = y’ instead of y.</p>

<p>Adversarial perturbation can then be categorized into one-step and multi-step perturbation. As the names imply, the one-step perturbation only involves a single stage — add noise once and that is it. On the other hand, the multi-step perturbation is an iterative attack that makes small modifications to the input each time. Therefore, the one-step attack is fast but excessive noise may be added, hence making it easier for humans to detect the changes. Furthermore, it places more weight on the objective of maximizing loss between H(x) and H(x’) and less on minimizing the amount of perturbation. Conversely, the multi-step attack is more strategic as it introduces small amounts of perturbation at each time. However, this also means such an attack is computationally more expensive.</p>

<h1 id="5-black-box-vs-white-box-attacks">5. Black Box VS White Box Attacks</h1>
<p>Now that we have looked at how adversarial attacks are generated, some astute readers may realize one fundamental assumption these attacks take on — that the attack target prediction model, H, is known to the adversary. Only when the targeted model is known can it be compromised to generate adversarial examples by changing the input. However, attackers do not always know or have access to the targeted model. This may sound like a surefire way to ward off these adversarial attackers, but the truth is that black box attacks are also highly effective.
Black box attacks are based on the notion of transferability of adversarial examples — the phenomenon whereby adversarial examples, although generated to attack a surrogate model G, can achieve impressive results when attacking another model H. The steps taken are as follows:</p>
<ol>
  <li>The attack target prediction model H is privately trained and unknown to the adversary.</li>
  <li>A surrogate model G, which mimics H, is used to generate adversarial examples.</li>
  <li>By using the transferability of adversarial examples, black box attacks can be launched to attack H.</li>
</ol>

<p>This attack can be launched either with the training dataset being known or unknown. In the case where the dataset is known to the adversary, the model G can be trained on the same dataset as model H to mimic H.</p>

<p>When the training dataset is unknown however, adversaries can leverage on Membership Inference Attacks, whereby an attack model whose purpose is to distinguish the target model’s behavior on the training inputs from its behavior on the inputs that it did not encounter during training is trained. In essence, this turns into a classification problem to recognize differences in the target model’s predictions on the inputs that it trained on versus the inputs that it did not train on. This enables the adversary to obtain a better sense of the training dataset D which model H was trained on, enabling the attacker to generate a shadow dataset S on the basis of the true training dataset so as to train the surrogate model G. Having trained G on S where G mimics H and S mimics D, black box attacks can then be launched on H.</p>

<h2 id="51-black-box-attacks">5.1 Black Box Attacks</h2>
<p>Now that we have seen how black box attacks vary from white box attacks in that the target model H is unknown to the adversary, we will cover the various tactics used in black box attacks.</p>

<h2 id="52-white-box-attacks">5.2 White Box Attacks</h2>

<h2 id="53-physical-attacks">5.3 Physical Attacks</h2>
<p>One simple way in which the query input is changed from x to x’ is by simply adding something physically (eg. bright colour) to disturb the model. One example is how researchers at CMU added eyeglasses to a person in an attack against facial recognition models. The image below illustrates the attack:</p>

<p><img src="/assets/images/adversarial-attack/2.png" alt="image" /></p>

<p>The first row of images correspond to the original image modified by adding the eyeglasses, and the second row of images correspond to the impersonation targets, which are the intended misclassification targets. Just by adding the eyeglasses onto the original image, the facial recognition model was tricked into classifying the images on the top row as the images in the bottom row.</p>

<p>Another example comes from researchers at Google who added stickers to the input image to change the classification of the image, as illustrated by the image below:
<img src="/assets/images/adversarial-attack/3.png" alt="image" /></p>

<p>These examples show how effective such physical attacks can be.</p>

<h2 id="54-out-of-distribution-ood-attack">5.4 Out of Distribution (OOD) Attack</h2>
<p>Another way in which black box attacks are carried out is through out-of-distribution (OOD) attacks. The traditional assumption in machine learning is that all train and test examples are drawn independently from the same distribution. In an OOD attack, this assumption is exploited by providing images of a different distribution from the training dataset to the model, for example feeding TinyImageNet data into a CIFAR-10 classifier which would lead to an incorrect prediction with high confidence.</p>

<h1 id="6-how-can-we-trust-machine-learning">6. How Can We Trust Machine Learning?</h1>
<p>Now that we have taken a look at the various types of adversarial attacks, a natural question then comes — how can we trust our machine learning models if they are so susceptible to adversarial attacks?</p>

<p>One possible approach has been proposed by Chow et al. in 2019 in the paper titled “Denoising and Verification Cross-Layer Ensemble Against Black-box Adversarial Attacks”. The approach is centred around enabling machine learning systems to automatically detect adversarial attacks and then automatically repair them through the use of denoising and verification ensembles.</p>

<h1 id="7-denoising-ensembles">7. Denoising Ensembles</h1>
<p>First, input images have to pass through denoising ensembles that attempt different methods to remove any added noise to the image, for example adding Gaussian noise. Since the specific noise added to the image by the adversary is unknown to the defender, there is a need for an ensemble of denoisers to each attempt to remove each type of noise.</p>

<p>The image below shows the training process for the denoising autoencoder — the original image is injected with some noise that the attacker might inject, and the autoencoder tries to reconstruct the original uncorrupted image. In the training process, the objective is to reduce the reconstruction error between the reconstructed image and the original image.</p>

<p><img src="/assets/images/adversarial-attack/4.png" alt="image" /></p>

<p>By developing an ensemble of these autoencoders each trained to remove a specific type of noise, the hope is that the corrupted images would be sufficiently denoised such that it is close to the original uncorrupted image to allow for image classification.</p>

<h2 id="71-verification-ensemble">7.1 Verification Ensemble</h2>
<p>After the images have been denoised, they then go through a verification ensemble which reviews every denoised image produced by each denoiser and then classifies the denoised image. Each classifier in the verification ensemble classifies each denoised image, and the ensemble then votes to determine the final category the image belongs to. This means that although some images may not have been denoised the correct way in the denoising step, the verification ensemble votes on all the denoised images, thereby increasing the likelihood of making a more accurate prediction.</p>

<h2 id="72-diversity">7.2 Diversity</h2>
<p>Diversity of the denoisers and verifiers have found to be very important because firstly, adversarial attackers will get better at altering images so there is a need for a diverse group of denoisers that can handle a variety of corrupted images. Following this, there is also a need for verifiers to be diverse so they can generate a variety of classifications so that it would be difficult adversarial attackers to manipulate them just as how they have managed to manipulate normal classifiers that we trust and use so frequently in machine learning.</p>

<p>This remains an open problem because, after all these decisions by the various verifiers, there is still a final decision maker that needs to decide whose opinion to listen to. The final decision maker would need to preserve the diversity present in the ensemble, which is not an easy task to tackle.</p>

<h1 id="8-conclusion">8. Conclusion</h1>
<p>We have taken a look at various types of adversarial attacks as well as a promising method to defend against these attacks. This is definitely something to keep in mind when we implement machine learning models. Instead of blindly trusting the models to produce the correct results, we need to guard against these adversarial attacks and always think twice before we accept the decisions made by these models.</p>

<p>A huge thanks to Professor Liu for this enlightening keynote on this pressing problem in machine learning!</p>

<h1 id="references">References</h1>
<ol>
  <li><a href="https://arxiv.org/abs/1805.07984">I. J. Goodfellow, J. Shlens, και C. Szegedy, “Explaining and Harnessing Adversarial Examples”. arXiv, 2014.</a></li>
  <li><a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">Tensorflow blog tutorials</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Adversarial_machine_learning">Adverserial Machine Learning</a></li>
  <li><a href="https://openai.com/blog/adversarial-example-research/">Attacking Machine Learning
with Adversarial Examples</a></li>
  <li><a href="https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa">Breaking neural networks with adversarial attacks</a></li>
</ol>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><category term="ai" /><category term="machine-learning" /><category term="deep-learning" /><summary type="html"><![CDATA[1. Intro Big Data powered machine learning and deep learning has yielded impressive advances in many fields. One example is the release of ImageNet consisting of more than 15 million labelled high-resolution images of 22,000 categories which revolutionized the field of computer vision. State-of-the-art models have already achieved a 98% top-five accuracy on the ImageNet dataset, so it seems as though these models are foolproof and that nothing can go wrong.]]></summary></entry><entry><title type="html">The Subtle Art of Not Giving a Fuck</title><link href="http://localhost:4000/book/self-help/focus/study-lessens/2023/03/19/the-art-of-dont-giving-a-fuck.html" rel="alternate" type="text/html" title="The Subtle Art of Not Giving a Fuck" /><published>2023-03-19T13:11:32+03:30</published><updated>2023-03-19T13:11:32+03:30</updated><id>http://localhost:4000/book/self-help/focus/study-lessens/2023/03/19/the-art-of-dont-giving-a-fuck</id><content type="html" xml:base="http://localhost:4000/book/self-help/focus/study-lessens/2023/03/19/the-art-of-dont-giving-a-fuck.html"><![CDATA[]]></content><author><name>Ali N. Parizi</name></author><category term="book" /><category term="self-help" /><category term="focus" /><category term="study-lessens" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Installing Tensorflow with GPU Support</title><link href="http://localhost:4000/blog/ai/machine-learning/deep-learning/2023/03/19/installing-tensorflow-with-gpu.html" rel="alternate" type="text/html" title="Installing Tensorflow with GPU Support" /><published>2023-03-19T12:19:43+03:30</published><updated>2023-03-19T12:19:43+03:30</updated><id>http://localhost:4000/blog/ai/machine-learning/deep-learning/2023/03/19/installing-tensorflow-with-gpu</id><content type="html" xml:base="http://localhost:4000/blog/ai/machine-learning/deep-learning/2023/03/19/installing-tensorflow-with-gpu.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>

<p>The rise to prominence of deep learning over the past decade is spectacular. From dominating in almost every single competition with its innovative and groundbreaking technologies, it has also led to several new types of research and training methods. One of the most popular ways to handle deep learning models to solve complex computational problems is with the help of deep frameworks.</p>

<p>One such popular deep learning library to build and construct models to find solutions to numerous tasks is TensorFlow. TensorFlow is regarded as one of the best libraries to solve almost any question related to neural networks and deep learning. While this library performs effectively with most smaller and simpler datasets to achieve tasks on a CPU, its true power lies in the utilization of the Graphics Processing Unit (GPU).</p>

<p>The GPU improvises the performance of this deep learning framework to reach new heights and peaks. However, one of the most annoying issues that deep learning programmers, developers, and enthusiasts face is the trouble of CUDA errors. This experience is rather frustrating for most individuals because it is a common occurrence while dealing with deep learning models.</p>

<p>In this article, we will explore how to get the latest version of TensorFlow and stay updated with modern technology.</p>

<p>We will use Anacoda because it’s almost the best python environment for machine-learning operations. To get started, let’s install anacoda on your computer. You can skip this step if you already have installed Anacoda on your Ubuntu machine.</p>

<h1 id="2-anaconda">2. Anaconda</h1>
<p>Anaconda is a distribution of the Python and R programming languages for scientific computing (data science, machine learning applications, large-scale data processing, predictive analytics, etc.), that aims to simplify package management and deployment. The distribution includes data-science packages suitable for Windows, Linux, and macOS. It is developed and maintained by Anaconda, Inc., which was founded by Peter Wang and Travis Oliphant in 2012. As an Anaconda, Inc. product, it is also known as Anaconda Distribution or Anaconda Individual Edition, while other products from the company are Anaconda Team Edition and Anaconda Enterprise Edition, both of which are not free. For me and probably you and almost 90% of people, the free version is good and does the job well for us. Installing anaconda requires installing For Debian based distros (such as Ubuntu) run the command below:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>apt <span class="nb">install </span>libgl1-mesa-glx libegl1-mesa libxrandr2 libxrandr2 libxss1 libxcursor1 libxcomposite1 libasound2 libxi6 libxtst6
</code></pre></div></div>
<p>For installing Anaconda you can visit its official website <a href="https://www.anaconda.com/products/distribution">anaconda.com</a> and download the latest installer version or Run the command below:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>curl https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh | /bin/bash
</code></pre></div></div>

<p>Then follow the installation process to complete it. Close and re-open your terminal window for the installation to take effect, or enter the command source ~/.bashrc (or ~/.zshrc if you are using zsh) to refresh the terminal.</p>

<blockquote>
  <p><strong>Note</strong>: The installer prompts you to choose whether to initialize Anaconda Distribution by running <code class="language-plaintext highlighter-rouge">conda init</code>. Anaconda recommends entering “yes”. If you enter “no”, then conda will not modify your shell scripts at all. To initialize after the installation process is done, first run source [PATH TO CONDA]/bin/activate and then run <code class="language-plaintext highlighter-rouge">conda init</code>.</p>
</blockquote>

<h2 id="21-creating-conda-environment">2.1 Creating Conda Environment</h2>

<p>Create a new conda environment named tf with the following command.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>conda create <span class="nt">--name</span> tf <span class="nv">python</span><span class="o">=</span>3.9
</code></pre></div></div>
<p>You can deactivate and activate it with the following commands.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>conda deactivate
<span class="gp">$</span><span class="w"> </span>conda activate tf
</code></pre></div></div>

<blockquote>
  <p><strong>Note</strong>: After installing Anaconda, the default conda environment will automatically activated when you open a new terminal. I personally prefer not to activate the environment automatically. You can turn off this feature running <code class="language-plaintext highlighter-rouge">$ conda config --set auto_activate_base False</code>.</p>
</blockquote>

<h1 id="4-nvidia-driver-cuda-and-cudnn">4. Nvidia Driver, CUDA and cuDNN</h1>
<p>It is required you to install a proper Nvidia driver on your machine. If you haven’t installed the Nvidia driver on your machine use the command below to install the driver:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>apt <span class="nb">install </span>nvidia-driver-515
</code></pre></div></div>

<p>To confirm that it is installed properly run the command bellow:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>nvidia-smi
</code></pre></div></div>

<pre><code class="language-output">Mon Mar 19 12:19:49 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0  On |                  N/A |
|  0%   47C    P8    44W / 340W |   1325MiB / 10240MiB |      4%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1876      G   /usr/lib/xorg/Xorg                940MiB |
|    0   N/A  N/A      2034      G   /usr/bin/gnome-shell               48MiB |
|    0   N/A  N/A      3396      G   ...1/usr/lib/firefox/firefox      161MiB |
|    0   N/A  N/A      4658      G   ...816051303568945556,131072       42MiB |
|    0   N/A  N/A      4797      G   ...RendererForSitePerProcess      130MiB |
+-----------------------------------------------------------------------------+
</code></pre>

<p>Then install CUDA and cuDNN:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(tf) $</span><span class="w"> </span>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge <span class="nv">cudatoolkit</span><span class="o">=</span>11.2.2 <span class="nv">cudnn</span><span class="o">=</span>8.1.0
</code></pre></div></div>
<p>Configure the system paths. You can do it with the following command every time you start a new terminal after activating your conda environment.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(tf) $</span><span class="w"> </span><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:<span class="nv">$CONDA_PREFIX</span>/lib/
</code></pre></div></div>

<p>For your convenience it is recommended that you automate it with the following commands. The system paths will be automatically configured when you activate this conda environment.</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(tf) $</span><span class="w"> </span><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$CONDA_PREFIX</span>/etc/conda/activate.d
<span class="gp">(tf) $</span><span class="w"> </span><span class="nb">echo</span> <span class="s1">'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/'</span> <span class="o">&gt;</span> <span class="nv">$CONDA_PREFIX</span>/etc/conda/activate.d/env_vars.sh
</code></pre></div></div>

<p>In Ubuntu 22.04, we have to install NVCC as well:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">#</span><span class="w"> </span>Install NVCC
<span class="gp">(tf) $</span><span class="w"> </span>conda <span class="nb">install</span> <span class="nt">-c</span> nvidia cuda-nvcc<span class="o">=</span>11.3.58
<span class="gp">#</span><span class="w"> </span>Configure the XLA cuda directory
<span class="gp">(tf) $</span><span class="w"> </span><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$CONDA_PREFIX</span>/etc/conda/activate.d
<span class="gp">(tf) $</span><span class="w"> </span><span class="nb">printf</span> <span class="s1">'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\n'</span> <span class="o">&gt;</span> <span class="nv">$CONDA_PREFIX</span>/etc/conda/activate.d/env_vars.sh
<span class="gp">(tf) $</span><span class="w"> </span><span class="nb">source</span> <span class="nv">$CONDA_PREFIX</span>/etc/conda/activate.d/env_vars.sh
<span class="gp">#</span><span class="w"> </span>Copy libdevice file to the required path
<span class="gp">(tf) $</span><span class="w"> </span><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$CONDA_PREFIX</span>/lib/nvvm/libdevice
<span class="gp">(tf) $</span><span class="w"> </span><span class="nb">cp</span> <span class="nv">$CONDA_PREFIX</span>/lib/libdevice.10.bc <span class="nv">$CONDA_PREFIX</span>/lib/nvvm/libdevice/
</code></pre></div></div>

<h1 id="5-installing-tensorflow">5. Installing Tensorflow</h1>
<p>TensorFlow requires a recent version of pip, so upgrade your pip installation to be sure you’re running the latest version.</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(tf) $</span><span class="w"> </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip
<span class="gp">(tf) $</span><span class="w"> </span>pip <span class="nb">install </span>tensorflow
</code></pre></div></div>
<p>Verify the GPU setup:</p>
<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">(tf) $</span><span class="w"> </span>python3 <span class="nt">-c</span> <span class="s2">"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"</span>
</code></pre></div></div>
<p>If a list of GPU devices is returned, you’ve installed TensorFlow successfully.</p>

<pre><code class="language-output">[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
</code></pre>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://docs.anaconda.com/anaconda/install/index.html"><em>Installing Anaconda (anaconda.com)</em></a></li>
  <li><a href="https://www.tensorflow.org/install/pip"><em>Install TensorFlow with pip (tensorflow.org)</em></a></li>
</ul>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><category term="ai" /><category term="machine-learning" /><category term="deep-learning" /><summary type="html"><![CDATA[1. Intro]]></summary></entry><entry><title type="html">Bypass The Islamic Republic Again: Installing V2ray + XUI</title><link href="http://localhost:4000/blog/network/vpn/2022/10/31/v2ray-xui.html" rel="alternate" type="text/html" title="Bypass The Islamic Republic Again: Installing V2ray + XUI" /><published>2022-10-31T13:10:23+03:30</published><updated>2022-10-31T13:10:23+03:30</updated><id>http://localhost:4000/blog/network/vpn/2022/10/31/v2ray-xui</id><content type="html" xml:base="http://localhost:4000/blog/network/vpn/2022/10/31/v2ray-xui.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>
<p>Last time, I wrote an article about bypassing God’s government restrictions to be able to access the outside world. Now the base solution still stands, but the protocol we used to implement that solution no longer works. We can clap our hands to the government priests for upgrading their knowledge about VPN protocols and say bravo in chinese “<a href="https://translate.google.com/?sl=zh-CN&amp;tl=en&amp;text=%E9%AB%98%E4%B8%9D%E7%BA%B3%E7%BA%B3%E9%A1%BF&amp;op=translate" title="Kose na na ton!">高丝纳纳顿</a>”.</p>

<p>As our government and Chinese folks are each other’s besties and in the same bed! we can conclude that any protocol that works on the great firewall of china, should work for the evils firewall of the Islamic Republic!</p>

<p>V2Ray is a VPN protocol written with love by some free Chinese people. It works on the application layer and its traffic looks like working with an actual web-site and it’s less prone to detect. The problem with V2Ray is the english language support, documentations and weak client software which can easily be improved if their community decided to share their information and issues in english not in fucking chinees!</p>

<p>For this article we consider you have access to a Virtual Machine in the outside world. You can access that machine using SSH or you can easily install a cockpit to be able to access your Linux machine through the browser.</p>

<h2 id="11-installing-cockpit-optional">1.1 Installing Cockpit (Optional)</h2>
<p>To install Cockpit, connect to your machine using ssh and use the aptitude package manager to install cockpit:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>apt update <span class="nt">--yes</span>
<span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>apt upgrade <span class="nt">--yes</span>
<span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>apt <span class="nb">install </span>cockpit <span class="nt">--yes</span>
</code></pre></div></div>

<p>This should install the cockpit and dose all the configurations for you. The cockpit is a web-based control panel and it will run on port 9090 of the server so, you have to allow connecting this port if your firewall is enabled:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>ufw allow 9090
<span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>ufw disable
<span class="gp">$</span><span class="w"> </span><span class="nb">sudo </span>ufw <span class="nb">enable</span>
</code></pre></div></div>

<p>Then open a browser and type the address <code class="language-plaintext highlighter-rouge">YOUR_VM_IP:9090</code> to see the cockpit panel.
Username and password of cockpit panel are the same as your vm user for example:</p>

<pre><code class="language-txt">username: root
password: 123456
</code></pre>

<p align="center">
    <img width="80%" src="/assets/images/projects/v2ray/cockpit.png" />
</p>

<h1 id="2-installing-v2ray-using-x-ui">2. Installing V2ray using X-UI</h1>
<p>To install V2ray you can easily install X-UI panel on your machine and this will automatically install all necessary things for you. X-UI documentation could be found here: <a href="https://seakfind.github.io/2021/10/10/X-UI/"><strong>seakfind</strong></a></p>

<p>The installation process is pretty easy, just connect to your machine and install <code class="language-plaintext highlighter-rouge">socat</code> first:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>apt <span class="nb">install </span>curl socat <span class="nt">-y</span>
</code></pre></div></div>

<p>Then you can skip obtaining certificate steps and directly jump on installing the x-ui using its script:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>bash &lt;<span class="o">(</span>curl <span class="nt">-Ls</span> https://raw.githubusercontent.com/vaxilu/x-ui/master/install.sh<span class="o">)</span>
</code></pre></div></div>
<p>Then it will ask you to type <code class="language-plaintext highlighter-rouge">yes|no</code>, you type yes and press enter on any chinese message prompt it shows to you. That’s it, you can start  the panel by typing:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>x-ui start
</code></pre></div></div>

<p>It starts the X-UI panel on the port <code class="language-plaintext highlighter-rouge">54321</code> of server. Open a browser and type <code class="language-plaintext highlighter-rouge">YOUR_VM_IP:54321</code> in the address bar to see the panel. Default username and password are:</p>

<pre><code class="language-txt">username: admin
password: admin
</code></pre>

<p align="center">
    <img width="80%" src="/assets/images/projects/v2ray/xui.png" />
</p>

<p>Default language of x-ui is chaines, i use Google chrome and Google translate to translate its contexts to english and i recommend you to do so.</p>

<p>After login, you have to go and change the default username and password of the panel (You can change the default port as well). From the sidebar panel select the third option, then select the second tab. Now type the old username and password on first two fields the new ones on the next two. Press <code class="language-plaintext highlighter-rouge">Revise</code> to save the changes.</p>

<p align="center">
    <img width="80%" src="/assets/images/projects/v2ray/x-ui-1.png" />
</p>

<p>Now go to inbound list from the sidebar and press add for creating a new VPN configuration. For example you can select VMess or VLess protocol under Websocket(ws) as you can see on the picture below. If you choose VLess be aware of that VLess has no encryption and VMess is a better choice.</p>

<blockquote>
  <p>Note: You can go and search for other V2Ray configurations but VMess is good enough for our article.</p>
</blockquote>

<p align="center">
    <img width="80%" src="/assets/images/projects/v2ray/xui-u.png" />
</p>

<p>Now, you are all set, you can scan the QR code or press copy share link to copy the share link to the clip board and send it to your clients.</p>

<p align="center">
    <img width="80%" src="/assets/images/projects/v2ray/x-ui-2.png" />
</p>

<h1 id="3-clients-setup-for-using-v2ray">3. Clients setup for using V2Ray</h1>
<p>To connect to the V2Ray server you have to install the proper client application on your devices. Here is a list of application clients for different devices and operating systems:</p>

<ul>
  <li><strong>Android</strong>: <a href="https://play.google.com/store/apps/details?id=com.v2ray.ang">V2rayNG (Google play)</a></li>
  <li><strong>IOS</strong>: <a href="https://apps.apple.com/us/app/napsternetv/id1629465476">NapsternetV (App Store)</a></li>
  <li><strong>Windows</strong>: <a href="https://github.com/Qv2ray/Qv2ray/releases/download/v2.7.0/Qv2ray-v2.7.0-Windows-Installer.exe">Qv2ray-v2.7.0-Windows-Installer.exe</a></li>
  <li><strong>Mac OSX</strong>: <a href="https://github.com/Qv2ray/Qv2ray/releases/download/v2.7.0/Qv2ray-v2.7.0-macOS-x64.dmg">Qv2ray-v2.7.0-macOS-x64.dmg</a></li>
  <li><strong>Linux</strong>: <a href="https://github.com/Qv2ray/Qv2ray/releases/download/v2.7.0/Qv2ray-v2.7.0-linux-x64.AppImage">Qv2ray-v2.7.0-linux-x64.AppImage</a></li>
</ul>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://seakfind.github.io/2021/10/10/X-UI/">https://seakfind.github.io/2021/10/10/X-UI/</a></li>
</ul>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><category term="network" /><category term="vpn" /><summary type="html"><![CDATA[1. Intro Last time, I wrote an article about bypassing God’s government restrictions to be able to access the outside world. Now the base solution still stands, but the protocol we used to implement that solution no longer works. We can clap our hands to the government priests for upgrading their knowledge about VPN protocols and say bravo in chinese “高丝纳纳顿”.]]></summary></entry><entry><title type="html">Variational AutoEncoders: An Introduction</title><link href="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/10/11/vae-intro.html" rel="alternate" type="text/html" title="Variational AutoEncoders: An Introduction" /><published>2022-10-11T08:24:05+03:30</published><updated>2022-10-11T08:24:05+03:30</updated><id>http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/10/11/vae-intro</id><content type="html" xml:base="http://localhost:4000/blog/ai/machine-learning/deep-learning/2022/10/11/vae-intro.html"><![CDATA[<h1 id="1-intro">1. Intro</h1>
<p>In machine learning, a Variational AutoEncoder, is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling, belonging to the families of probabilistic graphical models and variational Bayesian methods. Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders allow statistical inference problems to be rewritten as statistical optimization problems. They are meant to map the input variable to a multivariate latent distribution. Although this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning (<a href="https://en.wikipedia.org/wiki/Variational_autoencoder">Wikipedia</a>).</p>

<div align="center">
    <img src="/assets/images/blog/vae-intro/authors.png" />
    <br />
    <span>Diederik P. Kingma and Max Welling</span>
</div>

<h1 id="2-the-idea">2. The Idea</h1>

<h1 id="3-architecture">3. Architecture</h1>

<h1 id="4-implementing-vae-using-tensorflow">4. Implementing VAE using Tensorflow</h1>

<h1 id="5">5.</h1>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://arxiv.org/abs/1906.02691"><em>Diederik P. Kingma, &amp; Max Welling (2019). An Introduction to Variational Autoencoders. Foundations and Trends® in Machine Learning, 12(4), 307–392.</em></a></li>
</ul>]]></content><author><name>Ali N. Parizi</name></author><category term="blog" /><category term="ai" /><category term="machine-learning" /><category term="deep-learning" /><summary type="html"><![CDATA[1. Intro In machine learning, a Variational AutoEncoder, is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling, belonging to the families of probabilistic graphical models and variational Bayesian methods. Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders allow statistical inference problems to be rewritten as statistical optimization problems. They are meant to map the input variable to a multivariate latent distribution. Although this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning (Wikipedia).]]></summary></entry><entry><title type="html">Getting started with game hacking: Making a CS:GO cheat</title><link href="http://localhost:4000/project/security/cracking/game-hacking/2022/10/09/csgo-cheat.html" rel="alternate" type="text/html" title="Getting started with game hacking: Making a CS:GO cheat" /><published>2022-10-09T12:21:13+03:30</published><updated>2022-10-09T12:21:13+03:30</updated><id>http://localhost:4000/project/security/cracking/game-hacking/2022/10/09/csgo-cheat</id><content type="html" xml:base="http://localhost:4000/project/security/cracking/game-hacking/2022/10/09/csgo-cheat.html"><![CDATA[<h1 id="intro">Intro</h1>
<p>This project is an example of game hacking, and it’s created to share the personal experience with other enthusiasts in this branch of software security.
This project is just for educational proposes and i’m not responsible for any other subversive applications.
As you know game hacking in online games is not allowed owing to annoying some other people. Normal players who just want to have fun with games, don’t want to face a demon hacker in their fun times, so game developers will respond if you do something to annoy other players. In other words cheating in online games will cause your account be banned (e.g VACation on steam platform).</p>]]></content><author><name>Ali N. Parizi</name></author><category term="project" /><category term="security" /><category term="cracking" /><category term="game-hacking" /><summary type="html"><![CDATA[Intro This project is an example of game hacking, and it’s created to share the personal experience with other enthusiasts in this branch of software security. This project is just for educational proposes and i’m not responsible for any other subversive applications. As you know game hacking in online games is not allowed owing to annoying some other people. Normal players who just want to have fun with games, don’t want to face a demon hacker in their fun times, so game developers will respond if you do something to annoy other players. In other words cheating in online games will cause your account be banned (e.g VACation on steam platform).]]></summary></entry><entry><title type="html">Design patterns: Building Extensible and Maintainable Object-Oriented Software</title><link href="http://localhost:4000/book/programming/design/design-pattern/2022/10/09/design-patterns.html" rel="alternate" type="text/html" title="Design patterns: Building Extensible and Maintainable Object-Oriented Software" /><published>2022-10-09T03:21:13+03:30</published><updated>2022-10-09T03:21:13+03:30</updated><id>http://localhost:4000/book/programming/design/design-pattern/2022/10/09/design-patterns</id><content type="html" xml:base="http://localhost:4000/book/programming/design/design-pattern/2022/10/09/design-patterns.html"><![CDATA[]]></content><author><name>Ali N. Parizi</name></author><category term="book" /><category term="programming" /><category term="design" /><category term="design-pattern" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Clean code: A handbook of agile software craftsmanship</title><link href="http://localhost:4000/book/programming/engineering/management/ethical-engineering/2022/10/09/clean-code.html" rel="alternate" type="text/html" title="Clean code: A handbook of agile software craftsmanship" /><published>2022-10-09T02:22:17+03:30</published><updated>2022-10-09T02:22:17+03:30</updated><id>http://localhost:4000/book/programming/engineering/management/ethical-engineering/2022/10/09/clean-code</id><content type="html" xml:base="http://localhost:4000/book/programming/engineering/management/ethical-engineering/2022/10/09/clean-code.html"><![CDATA[]]></content><author><name>Ali N. Parizi</name></author><category term="book" /><category term="programming" /><category term="engineering" /><category term="management" /><category term="ethical-engineering" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">When: The scientific secrets of perfect timing</title><link href="http://localhost:4000/book/self-help/focus/study-lessens/2022/10/09/when.html" rel="alternate" type="text/html" title="When: The scientific secrets of perfect timing" /><published>2022-10-09T01:21:12+03:30</published><updated>2022-10-09T01:21:12+03:30</updated><id>http://localhost:4000/book/self-help/focus/study-lessens/2022/10/09/when</id><content type="html" xml:base="http://localhost:4000/book/self-help/focus/study-lessens/2022/10/09/when.html"><![CDATA[]]></content><author><name>Ali N. Parizi</name></author><category term="book" /><category term="self-help" /><category term="focus" /><category term="study-lessens" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Deep work: Rules for focused success in a distracted world</title><link href="http://localhost:4000/book/self-help/focus/study-lessens/2022/10/07/deep-work.html" rel="alternate" type="text/html" title="Deep work: Rules for focused success in a distracted world" /><published>2022-10-07T13:11:32+03:30</published><updated>2022-10-07T13:11:32+03:30</updated><id>http://localhost:4000/book/self-help/focus/study-lessens/2022/10/07/deep-work</id><content type="html" xml:base="http://localhost:4000/book/self-help/focus/study-lessens/2022/10/07/deep-work.html"><![CDATA[]]></content><author><name>Ali N. Parizi</name></author><category term="book" /><category term="self-help" /><category term="focus" /><category term="study-lessens" /><summary type="html"><![CDATA[]]></summary></entry></feed>